{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7692fcd",
   "metadata": {},
   "source": [
    "# [MICRO-452:][MICRO-452] Project Report - Groupe XX\n",
    "\n",
    "\n",
    "\n",
    "<p><b>Authors:</b>  Stephen Monnet, David Rüegg, Julien Burkhard, Sylvain Jacquart<br>\n",
    "<b>Supervisors:</b> Prof. Francesco Mondada, Laila El-Hamamsy<br>\n",
    "<b>Due date:</b>    December 12th, 2021</p>\n",
    "\n",
    "[MICRO-452]: https://moodle.epfl.ch/pluginfile.php/2727652/mod_resource/content/3/Week%209%20-%20Project%20Presentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a39a58",
   "metadata": {},
   "source": [
    "<h1><center> Thyminotaur - εσκαπινγ θε μαζε (Εscaping the maze) </center></h1>\n",
    "<img src=\"images/angry_thymio.gif\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3623776",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1><br>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Project-summary-,-hardware-and-choices\" data-toc-modified-id=\"Project-summary-,-hardware-and-choices-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Project summary, hardware and choices</a></li></ul><ul class=\"toc-item\"><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Motion-control\" data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Main\" data-toc-modified-id=\"Main-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Main</a></ul><ul class=\"toc-item\"><li><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9.&nbsp;&nbsp;</span>Conclusion</a></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2aea18",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "<a class=\"anchor\" id=\"Introduction\"></a>\n",
    "<p style='text-align: justify;'> \n",
    "    This mobile robotics project is inspired from the old legend of <i>Theseus and the Labyrinth</i>. It all started during the SHS course <i>Myths of the ancient Mediterranean Sea</i> with the following question:</p>\n",
    "\n",
    "**What if the Minotaur, trapped forever in the vast Labyrinth built on Creta island, could find his way out and live his best life under the greek sun?**\n",
    "\n",
    "The major goal of this project is therefore to help the little Thyminotaur to find his way out of the maze! To help him with his quest for freedom, the 5 following tools are implemented in the project.  \n",
    "\n",
    "1. **Vision**: Placed above the maze, a camera detects the walls, maze exits and the position of the robot. The optimal path is then computed with the vision information.\n",
    "2. **Global Navigation**: With the help of the camera, the global path to the nearest exit of the labyrinth is compiled and send as the route to follow for the Thymio.\n",
    "3. **Local Navigation**: With the help of the 5 front infrared sensors, the Thymio can avoid unexpected obstacles along the way, using inputs from a 5-neural network, each one associated with one sensor.\n",
    "4. **Filtering**: Extended Kalman Filter.\n",
    "5. **Motion Control**: Once the path is computed, the information is transmitted to the Thymio which starts moving.\n",
    "\n",
    "\n",
    "\n",
    "The main folder of the project can be found following `this link`.\n",
    "\n",
    "The global decision tree for the project can be summarize in the figure below.\n",
    "\n",
    "<img src=\"images/Decision_tree.PNG\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142ac74",
   "metadata": {},
   "source": [
    "# II. Project summary, hardware and choices\n",
    "<a class=\"anchor\" id=\"Project-summary-,-hardware-and-choices\"></a>\n",
    "### Project summary\n",
    "\n",
    "<img src=\"images/GlobalExplenations.PNG\" style=\"width: 600px;\">\n",
    "\n",
    "1. We place the Thymio on a random cell of the labyrinth. The camera detects the *ArUco Marker* on the Thymio, compiling its position and orientation. \n",
    "\n",
    "\n",
    "2. The labyrinth is drawn with 2 exits; using the Dijkstra’s algorithm, the Thymio follows the path to the nearest exit of the maze (green path on the image).\n",
    "\n",
    "\n",
    "3. As the robot almost reaches the exit, a mysterious old Greek god closes the wall in front of the poor Thyminotaur! Using the global vision, a new path is computed, and the robot turns to follow this alternative route (orange path on the image).\n",
    "\n",
    "\n",
    "4. During the path following, some unexpected obstacles will block the road to our adventurous Thyminotaur. Thanks to his local avoidance abilities (based on his proximity sensors/neural network architecture combination), no unexpected event will stand between him and the exit. He bypasses every obstacle until the end of his journey.\n",
    "\n",
    "\n",
    "5. The Thymio stops at the exit of the Labyrinth, satisfied with his regained freedom.\n",
    "\n",
    "### Hardware\n",
    "<p style='text-align: justify;'>\n",
    "The labyrinth covers a 4x5 cells unit (17cm x 20cm each) and is printed on an A0 white paper (shout-out to the PolyRepro for their fast delivery). To ensure a good camera tracking, the Thymio carries an <i> ArUco Marker</i>, which allows to detect the position and orientation of the robot. The marker is stick to <i>Lego bricks</i> and mounted directly on top of the Thymio, as you can see on the picture below.</p>\n",
    "\n",
    "<img src=\"images/combined_pic_thymio_lab.PNG\" style=\"width: 600px; \">\n",
    "\n",
    "\n",
    "### Relevant choices made for this project\n",
    "1. At first, a solid labyrinth made of wood was considered enclosing the Thymio. After several tries and prototypes, the full paper option (black lines printed on a large white sheet) was selected for its practicality and easiness of implementation with the vision part.\n",
    "\n",
    "    \n",
    "2. To mark out the available space of the Thymio (his playground), 4 additional arUco Markers are placed on each corner of the sheet. The camera placed above the labyrinth is never perfectly aligned with the sheet, both in position and angle; the arUcodes allow a straightforward flattening of the image at the vision initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68597e1",
   "metadata": {},
   "source": [
    "# III. Vision\n",
    "<a class=\"anchor\" id=\"Vision\"></a>\n",
    "\n",
    "In the Thyminotaur project, the whole vision data is provided by a single static camera located over the robot's scope of operation. We use vision during the initialization phase (labyrinth recognition, full-scale calibration, corners detection) and while the robot is moving (Thymio detection). We coded the entire vision part using the Python library <i>Open Source Computer Vision</i>, or [open cv][cv2] in short.\n",
    "\n",
    "We assigned arUco Markers to every critical variable of the project (the moving Thymio, the 4 corners) to ensure consistency in the code. These markers are a simple yet efficient way to convert information from a full vision scope to a code of variables and raw data (images on left & center). For corners, it is just a way to fix the operating frame of the robot. On the Thymio, the marker allows us to track the position and angle of the Thyminotaur from the starting point in real time.\n",
    "\n",
    "The labyrinth is made of large, 20mm-width black lines printed of white paper to achieve the best contrast. This simplifies our program in defining the possible paths for the robot (image on right). Since system vision is a key element in both the global navigation algorithm and for general system motion control, we directly included the vision functions in each class rather than putting them in their own separate class. <br>\n",
    "\n",
    "<img src=\"images/vision_3_image.PNG\" style=\"width: 900px;\">\n",
    "    \n",
    "The `detect_labyrinth` function below summarizes on how black lines on a A0 paper sheet can be recognized as a concrete object. This is just once relevant function of the vision part in this project; you can find more on the `vision_utilis.py` file\n",
    "    \n",
    "[cv2]: https://github.com/opencv/opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Gives a binary grid representation of the labyrinth (img must be grayscale)\n",
    "# ----\n",
    " \n",
    "def detect_labyrinth(img, wall_size):\n",
    "  h,w = img.shape  #(height and width parameters)\n",
    "\n",
    "  # Remove AruCo\n",
    "  detected = detect_aruco(img)\n",
    "  erase_aruco(img, detected)\n",
    "\n",
    "\n",
    "  # Threshold\n",
    "  _, th = cv.threshold(img,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
    "\n",
    "  # Remove noise\n",
    "  kernel = np.ones((10,10),np.uint8)\n",
    "  th = cv.morphologyEx(th, cv.MORPH_OPEN, kernel)\n",
    "\n",
    "  # Detect connected components\n",
    "  num_labels, labels, stats, _ = cv.connectedComponentsWithStats(th, 8, cv.CV_32S)\n",
    "  sort_ind = np.argsort(stats[:, cv.CC_STAT_AREA])\n",
    "\n",
    "  # Pick two biggest components\n",
    "  # -1 is all the components\n",
    "  result = np.zeros_like(th)\n",
    "  result[labels == sort_ind[-2]] = 255\n",
    "  result[labels == sort_ind[-3]] = 255\n",
    "\n",
    "  # Dilate walls to their real dimensions\n",
    "  kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE,(wall_size,wall_size))\n",
    "  result = cv.dilate(result,kernel,iterations = 1)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335f748",
   "metadata": {},
   "source": [
    "# IV. Global navigation\n",
    "<a class=\"anchor\" id=\"Global-navigation\"></a>\n",
    "Our global navigation implementation is based on the motion cost approach, as if some moves (turning, moving in diagonal) were more costly than others (moving straight).\n",
    "\n",
    "The path to follow is made of a succession of absolute points on the XY Cartesian plane. This is the most convenient way to navigate our Thymio inside the rectangle, straight design maze. \n",
    "\n",
    "<img src=\"images/Path_planning.PNG\" style=\"width: 900px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997e9dc",
   "metadata": {},
   "source": [
    "# V. Local Navigation\n",
    "<a class=\"anchor\" id=\"Local-navigation\"></a>\n",
    "The local navigation runs continually while the Thymio is moving. This section is only implemented to avoid unexpected obstacles along the way, not to reorientate the Thymio after such an avoidance. Therefore, both motors are controlled by the `obstacle_avoidance_full` routine below, over the global navigation algorithm. \n",
    "\n",
    "This function is straightforward. It consists of assigning different weights (`diffDelta`) to each of the 5 IR proximity sensors on Thymio's front. These weights give stronger importance to external sensors over the middle one.  Using a variable threshold (which can be tuned to match environment conditions), the Thymio is capable of detecting if an unexpected obstacle stands on his way to the exit:\n",
    "- When no obstacle is in sight, the robot is monitored by the global path navigation and goal tracking algorithm.\n",
    "- When something's on the way, the diffDelta variable increase the speed of the motor close to the obstacle side. The closer the obstacle, the faster the motor goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323197e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacle_avoidance_full(node, variables):\n",
    "    # global motor_left, motor_right\n",
    "    try:   \n",
    "        speed0 = 100       # nominal speed\n",
    "        speedGain = 5      # gain used with ground gradient\n",
    "        obstacleGain = 2   # gain used to avoid obstacle\n",
    "        obstThr = 20       # obstacle threshold to switch state 1->0\n",
    "        state = 0          # 0=gradient, 1=obstacle avoidance\n",
    "        prox = variables[\"prox.horizontal\"]\n",
    "\n",
    "        # difference between sensors\n",
    "        diffDelta = prox[0]+0.5*prox[1]-0.25*prox[2]-0.5*prox[3]-prox[4]\n",
    "\n",
    "        if state == 0 and (prox[0] > obstThr or prox[1] > obstThr or prox[2] > obstThr or prox[3] > obstThr or prox[4] > obstThr):\n",
    "        # switch from goal tracking to obst avoidance if obstacle detected\n",
    "           state = 1\n",
    "\n",
    "        elif state == 1 and (prox[0] < obstThr and prox[1] < obstThr and prox[2] < obstThr and prox[3] < obstThr and prox[4] < obstThr):\n",
    "        # switch from obst avoidance to goal tracking if obstacle got unseen\n",
    "             state = 0\n",
    "\n",
    "             if state == 0 :\n",
    "             # goal tracking: turn toward the goal\n",
    "                motor_left = speed0 - speedGain * diffDelta\n",
    "                motor_right = speed0 + speedGain * diffDelta           \n",
    "                \n",
    "             else :\n",
    "             # obstacle avoidance: accelerate wheel near obstacle\n",
    "                motor_left = speed0 + obstacleGain* diffDelta\n",
    "                motor_right = speed0 + obstacleGain* (-diffDelta)\n",
    "\n",
    "        node.send_set_variables(motors(motor_left, motor_right))\n",
    "    except KeyError:\n",
    "        pass  # prox.horizontal not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a67046",
   "metadata": {},
   "source": [
    "# VI. Filtering\n",
    "<a class=\"anchor\" id=\"Filtering\"></a>\n",
    "We choose to tackle the filtering part of this project by implementing an Extended Kalman Filter.\n",
    "\n",
    "\n",
    "7.8/21\n",
    "estimer le probabilité\n",
    "distri de proba, à ajuster en continu. Quand il avance dans l'ombre (sans cam), plus la gaussienne s'applatit.\n",
    "Camera = sensor absolu, pas d'incertitude sur la position / sans cam = accéléromètre, vitesse -> sensor relatif\n",
    "Kalmann ->, général, Gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c984d1",
   "metadata": {},
   "source": [
    "# VII. Motion Control\n",
    "<a class=\"anchor\" id=\"Motion-control\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d026996",
   "metadata": {},
   "source": [
    "The motion control aims to provide the correct motors command to the robot, depending on its position (x, y, orientation) and the path to follow computed previously. The control approach is directly based on the fact that the trajectories can only be composed of straight lines in the actual maze. Therefore the robot can accurately follow the path simply by moving to the intersection points of the straight lines generated by the Dijkstra’s algorithm.\n",
    "The motors command can be decomposed into three terms :\n",
    "\n",
    "$$\\vec v_{mot} = \\vec v_{orientation} + \\vec v_{position} + \\vec v_{obstacle}$$\n",
    "\n",
    "1. **Orientation**: When the next point $P_{k+1}$ where the robot needs to go is known, the robot has to correct its orientation $\\alpha_r$. A simple P regulator is used to miminimze the error $\\alpha_e = \\alpha_r - \\alpha_c$, where $\\alpha_c$ is the commanded orientation of the robot, computed with : \n",
    "\n",
    "$$\\alpha_c = -\\frac{180}{\\pi} \\cdot \\tan^{-1}(\\frac{Py_{k+1} - Py_{k}}{Px_{k+1} - Px_{k}})$$\n",
    "\n",
    "    The orientation part of the commanded speed is then : \n",
    "\n",
    "$$\\vec v_{orientation} = \\pm K_{p_\\alpha} \\cdot \\alpha_e$$\n",
    "    \n",
    "    The $\\pm$ sign is used to notify that the robot needs to turn, so the speed of the left and the right wheel need to be in the opposite directions.\n",
    "\n",
    "\n",
    "2. **Position**: Now that the robot has the right orientation, it needs to move forward to reach the point $P_{k+1}. The motor command for this part is not computed using a classic P control law because it is based on the angular error. The objective is to have a constant speed when the robot is travelling between two points with the right orientation but also to have a speed close to zero (for this part) when the orientation is wrong : \n",
    "\n",
    "$$\\vec v_{position} = K_{p_d} \\cdot (180 - \\mid\\alpha_e\\mid)$$\n",
    "\n",
    "\n",
    "\n",
    "3. **Obstacle avoidance**: With the help of the camera, the global path to the nearest exit of the labyrinth is compiled and send as the route to follow for the Thymio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab0254",
   "metadata": {},
   "source": [
    "# VIII . Main\n",
    "<a class=\"anchor\" id=\"Main\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ebf7f",
   "metadata": {},
   "source": [
    "# IX. Conclusion\n",
    "<a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
