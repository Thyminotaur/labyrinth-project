{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7692fcd",
   "metadata": {},
   "source": [
    "# [MICRO-452:][MICRO-452] Project Report - Groupe XX\n",
    "\n",
    "\n",
    "\n",
    "<p><b>Authors:</b>  Stephen Monnet, David Rüegg, Julien Burkhard, Sylvain Jacquart<br>\n",
    "<b>Supervisors:</b> Prof. Francesco Mondada, Laila El-Hamamsy<br>\n",
    "<b>Due date:</b>    December 12th, 2021</p>\n",
    "\n",
    "[MICRO-452]: https://moodle.epfl.ch/pluginfile.php/2727652/mod_resource/content/3/Week%209%20-%20Project%20Presentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a39a58",
   "metadata": {},
   "source": [
    "<h1><center> Thyminotaur - εσκαπινγ θε μαζε (Εscaping the maze) </center></h1>\n",
    "<img src=\"images/angry_thymio.gif\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3623776",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1><br>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Project-summary-,-hardware-and-choices\" data-toc-modified-id=\"Project-summary-,-hardware-and-choices-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Project summary, hardware and choices</a></li></ul><ul class=\"toc-item\"><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Motion-control\" data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Main\" data-toc-modified-id=\"Main-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Main</a></ul><ul class=\"toc-item\"><li><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9.&nbsp;&nbsp;</span>Conclusion</a></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2aea18",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "<a class=\"anchor\" id=\"Introduction\"></a>\n",
    "<p style='text-align: justify;'> \n",
    "    This mobile robotics project is inspired form the old legend of <i>Theseus and the Labyrinth</i>. It all started during the SHS course <i>Myths of the ancient Mediterranean Sea</i> with the following question:</p>\n",
    "\n",
    "**What if the Minotaur, trapped forever in the huge Labyrinth built on Creta island, could find his way out and live his best life under the greek sun?**\n",
    "\n",
    "The main goal of this project is therefore to help the little Thyminotaur to find his way out of the maze! To help him with his quest of freedom, the 5 following tools are implemented in the project.  \n",
    "\n",
    "1. Vision: Placed well above the maze, a camera detects the walls, maze's exits and the position of the robot. The optimal path is then computed with these vision information.\n",
    "2. Global Navigation: With the help of the camera, the global path to the nearest exit of the labyrinth is compiled and send as the route to follow for the Thymio.\n",
    "3. Local Navigation: With the help of the 5 front infrared sensors, the Thymio can avoid unexpected obstacles along the way, using inputs from a 5-neural network, each one associated to one sensor.\n",
    "4. Filtering: TO BE COMPLETED.\n",
    "5. Motion Control: Once the path is computed, the information is transmitted to the Thymio which starts moving.\n",
    "\n",
    "\n",
    "\n",
    "The main folder of the project can be found following `This link`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142ac74",
   "metadata": {},
   "source": [
    "# II. Project summary, hardware and choices\n",
    "<a class=\"anchor\" id=\"Project-summary-,-hardware-and-choices\"></a>\n",
    "### Project summary\n",
    "\n",
    "<img src=\"images/GlobalExplenation.PNG\" style=\"width: 600px;\">\n",
    "\n",
    "1. The Thymio is placed on a random cell of the labyrinth. The camera detects the *ArUco Marker* on the Thymio, compilling its position and orientation. \n",
    "\n",
    "\n",
    "2. The labyrinth is drawn with 2 exits; using the Dijkstra’s algorithm, the Thymio follows the path to the nearest exit of the maze (green path on the image).\n",
    "\n",
    "\n",
    "3. As the robot almost reaches the exit, a mysterious old greek god closes the wall in front of the poor Thyminotaur! Using the global vision, a new path is computed and the robot turns to follow this new route (orange path on the image).\n",
    "\n",
    "\n",
    "4. During the path following, some unexpected obstacles will block the road to our adventurous Thyminotaur. Thanks to his local avoidance abilities (based on his proximity sensors/neural network architecture combination), no unexpected event will stand between him and the exit. He bypasses every obstacles until the end of his journey.\n",
    "\n",
    "\n",
    "5. The Thymio stops at the exit of the Labyrinth, satisfied with his regained freedom.\n",
    "\n",
    "### Hardware\n",
    "<p style='text-align: justify;'>\n",
    "The labyrinth covers a 4x5 cells unit (17cm x 20cm each) and is printed on a A0 white paper (shout-out to the PolyRepro for their fast delivery). To ensure a good camera tracking, the Thymio carries an <i> ArUco Marker</i>, which allows to detect the position and orientation of the robot. The marker is stick to <i>Lego bricks</i> and mounted directly on top of the Thymio, as you can see on the picture below.</p>\n",
    "\n",
    "<img src=\"images/combined_pic_thymio_lab.PNG\" style=\"width: 600px;\">\n",
    "\n",
    "\n",
    "### Relevant choices made for this project\n",
    "1. At first, a solid labyrinth made of wood was considered to enclose the Thymio. After several tries and prototypes, the full paper option (black lines printed on a large white sheet) was selected for its practicality and easiness of implementation with the vision part.\n",
    "\n",
    "    \n",
    "2. To mark out the available space of the Thymio (his playground), 4 additionnal arUco Markers are placed on each corner of the sheet. The camera placed above the labyrinth is never perfectly aligned with the sheet, both in position and angle; the arUcodes allow a straightforward flattening of the image at the vision initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68597e1",
   "metadata": {},
   "source": [
    "# III. Vision\n",
    "<a class=\"anchor\" id=\"Vision\"></a>\n",
    "\n",
    "In the Thyminotaur project, the whole vision data is provided by a single, static camera located over the robot's scope of operation. We use vision during the initialization phase (labyrinth recognition, full-scale calibration, corners detection) and while the robot is moving (path computation, exit and thymio detection). The entire vision part is coded using the Python library <i>Open Source Computer Vision</i>, or [open cv][cv2] in short.\n",
    "\n",
    "We assignated arUco Markers to every critical variables of the project (the moving Thymio, the 4 corners, the 2 possible exits) to ensure consistency in the code. These markers are a simple yet efficent way to convert information from a full vision scope to a code of variables and raw data (images on left & center). For corners, it is just a way to fix the oprating frame of the robot. On the Thymio, the marker allows us to track the position and angle of the Thyminotaur, from the strating point in real time.\n",
    "\n",
    "The labyrinth is made of large, 20mm-width black lines printed of white paper to achieve the best contrast. This simplifies the task of our programm when it comes to define the possible paths for the robot (image on right). Since system vision is a key element in both the global avoidance algorithme as well as for general system motion control, we directly included the appropriate vision functions in each class rather than putting them in their own seperate class. <br>\n",
    "\n",
    "<img src=\"images/vision_3_image.PNG\" style=\"width: 900px;\">\n",
    "    \n",
    "The `detect_labyrinth` function below gives an overview on how black lines on a A0 paper sheet can be recognized as a concrete object. This is just once relevant function of the vision part in this project; you can find more on the `vision_utilis.py` file\n",
    "    \n",
    "[cv2]: https://github.com/opencv/opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Gives a binary grid representation of the labyrinth (img must be grayscale)\n",
    "# ----\n",
    " \n",
    "def detect_labyrinth(img, wall_size):\n",
    "  h,w = img.shape  #(height and width parameters)\n",
    "\n",
    "  # Remove AruCo\n",
    "  detected = detect_aruco(img)\n",
    "  erase_aruco(img, detected)\n",
    "\n",
    "\n",
    "  # Threshold\n",
    "  _, th = cv.threshold(img,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
    "\n",
    "  # Remove noise\n",
    "  kernel = np.ones((10,10),np.uint8)\n",
    "  th = cv.morphologyEx(th, cv.MORPH_OPEN, kernel)\n",
    "\n",
    "  # Detect connected components\n",
    "  num_labels, labels, stats, _ = cv.connectedComponentsWithStats(th, 8, cv.CV_32S)\n",
    "  sort_ind = np.argsort(stats[:, cv.CC_STAT_AREA])\n",
    "\n",
    "  # Pick two biggest components\n",
    "  # -1 is all the components\n",
    "  result = np.zeros_like(th)\n",
    "  result[labels == sort_ind[-2]] = 255\n",
    "  result[labels == sort_ind[-3]] = 255\n",
    "\n",
    "  # Dilate walls to their real dimensions\n",
    "  kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE,(wall_size,wall_size))\n",
    "  result = cv.dilate(result,kernel,iterations = 1)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335f748",
   "metadata": {},
   "source": [
    "# IV. Global navigation\n",
    "<a class=\"anchor\" id=\"Global-navigation\"></a>\n",
    "Our global navigation implementation is based on the motion cost approach, like if some moves (turning, moving in diagonal) were more costly than others (moving straight).\n",
    "\n",
    "The path to follow is made of a succession of absolutes points on the XY, cathesian plane. This is the most convenient way to navigate our Thymio inside the rectangle, straight design maze. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997e9dc",
   "metadata": {},
   "source": [
    "# V. Local Navigation\n",
    "<a class=\"anchor\" id=\"Local-navigation\"></a>\n",
    "The local navigtion runs continually while the Thymio is moving. This section is only implemented to avoid unexpected obstacles along the way, not to reorientate the Thymio after such an avoidance. Therefore, both motors are controlled by the `obstacle_avoidance_full` routine below, over the global navigation algorithm. \n",
    "\n",
    "This function is quite straightforward. It consist in assigning different weights (`diffDelta`) to each of the 5 IR proximity sensors located on Thymio's front. These weight give stronger importance to external sensors over the middle one.  Using a variable threshold (which can be tuned to match environment conditions), the Thymio is capable to detect if an unexpected obstacle stands on his way to the exit:\n",
    "- When no obstacle is in sight, the robot is monitored by the global path navigation and goal tracking algorithm.\n",
    "- When something's on the way, the diffDelta variable increase the speed of the motor close to the obstacle side. The closer the obstacle, the faster the motor goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323197e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacle_avoidance_full(node, variables):\n",
    "    # global motor_left, motor_right\n",
    "    try:   \n",
    "        speed0 = 100       # nominal speed\n",
    "        speedGain = 5      # gain used with ground gradient\n",
    "        obstacleGain = 2   # gain used to avoid obstacle\n",
    "        obstThr = 20       # obstacle threshold to switch state 1->0\n",
    "        state = 0          # 0=gradient, 1=obstacle avoidance\n",
    "        prox = variables[\"prox.horizontal\"]\n",
    "\n",
    "        # difference between sensors\n",
    "        diffDelta = prox[0]+0.5*prox[1]-0.25*prox[2]-0.5*prox[3]-prox[4]\n",
    "\n",
    "        if state == 0 and (prox[0] > obstThr or prox[1] > obstThr or prox[2] > obstThr or prox[3] > obstThr or prox[4] > obstThr):\n",
    "        # switch from goal tracking to obst avoidance if obstacle detected\n",
    "           state = 1\n",
    "\n",
    "        elif state == 1 and (prox[0] < obstThr and prox[1] < obstThr and prox[2] < obstThr and prox[3] < obstThr and prox[4] < obstThr):\n",
    "        # switch from obst avoidance to goal tracking if obstacle got unseen\n",
    "             state = 0\n",
    "\n",
    "             if state == 0 :\n",
    "             # goal tracking: turn toward the goal\n",
    "                motor_left = speed0 - speedGain * diffDelta\n",
    "                motor_right = speed0 + speedGain * diffDelta           \n",
    "                \n",
    "             else :\n",
    "             # obstacle avoidance: accelerate wheel near obstacle\n",
    "                motor_left = speed0 + obstacleGain* diffDelta\n",
    "                motor_right = speed0 + obstacleGain* (-diffDelta)\n",
    "\n",
    "        node.send_set_variables(motors(motor_left, motor_right))\n",
    "    except KeyError:\n",
    "        pass  # prox.horizontal not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a67046",
   "metadata": {},
   "source": [
    "# VI. Filtering\n",
    "<a class=\"anchor\" id=\"Filtering\"></a>\n",
    "\n",
    "7.8/21\n",
    "estimer le probabilité\n",
    "distri de proba, à ajuster en continu. Quand il avance dans l'ombre (sans cam), plus la gaussienne s'applatit.\n",
    "Camera = sensor absolu, pas d'incertitude sur la position / sans cam = accéléromètre, vitesse -> sensor relatif\n",
    "Kalmann ->, général, Gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c984d1",
   "metadata": {},
   "source": [
    "# VII. Motion Control\n",
    "<a class=\"anchor\" id=\"Motion-control\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab0254",
   "metadata": {},
   "source": [
    "# VIII . Main\n",
    "<a class=\"anchor\" id=\"Main\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ebf7f",
   "metadata": {},
   "source": [
    "# IX. Conclusion\n",
    "<a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
