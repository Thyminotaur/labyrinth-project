{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7692fcd",
   "metadata": {},
   "source": [
    "# [MICRO-452:][MICRO-452] Project Report\n",
    "\n",
    "\n",
    "\n",
    "<p><b>Authors:</b> &nbsp;&emsp;&emsp;&emsp;&emsp;&emsp;Stephen Monnet, David Rüegg, Julien Burkhard, Sylvain Jacquart<br>\n",
    "<b>Supervisors:</b> &nbsp;&emsp;&emsp;&emsp;Prof. Francesco Mondada, Laila El-Hamamsy<br>\n",
    "<b>Due date:</b>  &nbsp;&nbsp;&nbsp;&emsp;&emsp;&emsp;&emsp;December 12th, 2021</p>\n",
    "<b>Presentation date:</b> &nbsp;&nbsp;December 16th, 2021</p>\n",
    "\n",
    "[MICRO-452]: https://moodle.epfl.ch/pluginfile.php/2727652/mod_resource/content/3/Week%209%20-%20Project%20Presentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a39a58",
   "metadata": {},
   "source": [
    "<h1><center> Thyminotaur - εσκαπινγ θε μαζε (Εscaping the maze) </center></h1>\n",
    "<img src=\"images/angry_thymio.gif\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3623776",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1><br>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Project-summary-,-hardware-and-choices\" data-toc-modified-id=\"Project-summary-,-hardware-and-choices-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Project summary, hardware and choices</a></li></ul><ul class=\"toc-item\"><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Motion-control\" data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Main\" data-toc-modified-id=\"Main-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Main</a></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2aea18",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "<a class=\"anchor\" id=\"Introduction\"></a>\n",
    "<p style='text-align: justify;'> \n",
    "    This mobile robotics project is inspired by the old legend of <i>Theseus and the Labyrinth</i>. It all started during the SHS course <i>Myths of the ancient Mediterranean Sea</i> with the following question:</p>\n",
    "\n",
    "**What if the Minotaur, trapped forever in the vast Labyrinth built on Creta island, could find his way out and live his best life under the greek sun?**\n",
    "\n",
    "The major goal of this project is therefore to help the little Thyminotaur to find his way out of the maze! To help him with his quest for freedom, the 5 following tools are implemented in the project.  \n",
    "\n",
    "1. **Vision**: Placed above the maze, a camera detects the walls, maze exits and the position of the robot. The optimal path is then computed with the vision information. We also perform calibration prior to the execution of the program, to guarantee an accurate tracking of the Thymio.\n",
    "\n",
    "\n",
    "2. **Global Navigation**: With the help of the camera, the global path to the nearest exit of the labyrinth is compiled and send as the route to follow for the Thymio. The nearest exit is detected with a common cost-function algorithm.\n",
    "\n",
    "\n",
    "3. **Local Navigation**: With the help of the 5 front infrared sensors, the Thymio can avoid unexpected obstacles along the way. We assign a different weight to each sensor, to ensure a smooth obstacle avoidance behaviour a minimum of wall-crossing during these manoeuvres.\n",
    "\n",
    "\n",
    "4. **Filtering**: An extended Kalman Filter is implemented in this project, as a support to the vision part. Using this filter, we can track the robot with a precision in linear speed/position of XX and in angular speed/angle of YY. <font color='red'>TO COMPLETE</font>\n",
    "\n",
    "\n",
    "5. **Motion Control**: Once the path is computed, the information is transmitted to the Thymio which starts moving.\n",
    "\n",
    "\n",
    "The main folder of the project can be found following the repository: https://github.com/jbyuki/labyrinth-project\n",
    "\n",
    "The global decision tree for the project can be summarized in the figure below. Our Thyminotaur is robust to a camera blinding (thanks to the Kalman filter) but not to kidnapping; after all, it is hard to kidnap robots surrounded by labyrinth walls...\n",
    "\n",
    "<img src=\"images/Decision_tree.PNG\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142ac74",
   "metadata": {},
   "source": [
    "# II. Project summary, hardware and choices\n",
    "<a class=\"anchor\" id=\"Project-summary-,-hardware-and-choices\"></a>\n",
    "### Project summary\n",
    "\n",
    "<img src=\"images/GlobalExplenation.PNG\" style=\"width: 600px;\">\n",
    "\n",
    "1. We place the Thymio on a random cell of the labyrinth. The camera detects the *ArUco Marker* on the Thymio, compiling its position and orientation. \n",
    "\n",
    "\n",
    "2. The labyrinth is drawn with 2 exits; using the A* algorithm, the Thymio follows the path to the nearest exit of the maze (green path on the figure).\n",
    "\n",
    "\n",
    "3. During the path following, unexpected obstacles will block the road to our adventurous Thyminotaur. Thanks to his local avoidance abilities (based on his proximity sensors/combination of weights), no unexpected event will stand between him and the exit. He bypasses every obstacle until the maze exit.\n",
    "\n",
    "\n",
    "4. The Thymio stops at the exit of the Labyrinth, satisfied with his regained freedom.\n",
    "\n",
    "### Hardware\n",
    "<p style='text-align: justify;'>\n",
    "The labyrinth covers a 4x5 cells unit (17cm x 20cm each) and is printed on an A0 white paper (shout-out to the PolyRepro for their fast delivery). To ensure a good camera tracking, the Thymio carries an <i> ArUco Marker</i>, which allows to detect the position and orientation of the robot. The marker is stuck to <i>Lego bricks</i> and mounted directly on top of the Thymio, as you can see in the picture below.</p>\n",
    "\n",
    "<img src=\"images/combined_pic_thymio_lab.PNG\" style=\"width: 600px; \">\n",
    "\n",
    "\n",
    "### Additional choices for this project\n",
    "1. At first, a solid labyrinth made of wood was considered enclosing the Thymio. After several tries and prototypes, the full paper option (black lines printed on a large white sheet) was selected for its practicality and easiness of implementation with the vision part.\n",
    "\n",
    "    \n",
    "2. To mark out the available space of the Thymio (his playground), 4 additional ArUco Markers are placed on each corner of the sheet. The camera placed above the labyrinth is never perfectly aligned with the sheet, both in position and angle; the ArUcodes, combined with extreme-positions calibration, allow a straightforward flattening of the image at the vision initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68597e1",
   "metadata": {},
   "source": [
    "# III. Vision\n",
    "<a class=\"anchor\" id=\"Vision\"></a>\n",
    "\n",
    "In the Thyminotaur project, the whole vision data is provided by a single static camera located over the robot's scope of operation. We use vision during the initialization phase (labyrinth recognition, full-scale calibration, corners detection) and while the robot is moving (Thymio detection). We coded the entire vision part using the Python library <i>Open Source Computer Vision</i>, or [opencv][cv2] in short.\n",
    "\n",
    "We assigned ArUco Markers to critical variables of the project (the moving Thymio and the 4 corners) to ensure consistency in the code. These markers are a simple yet efficient way to convert information from a full vision scope to a code of variables and raw data (images on left & center). For corners, it is just a way to fix the operating frame of the robot. On the Thymio, the marker allows to track the position and angle of the Thyminotaur from the starting point in real time.\n",
    "\n",
    "The labyrinth is made of large, 20mm-width black lines printed of white paper to achieve the best contrast. This simplifies our program in defining the possible paths for the robot (image on right). Since system vision is a key element in both the global navigation algorithm and for general system motion control, we directly included the vision functions in each class rather than putting them in their own separate class. <br>\n",
    "\n",
    "<img src=\"images/vision_3_image.PNG\" style=\"width: 900px;\">\n",
    "    \n",
    "The `detect_labyrinth` function below summarizes how black lines, on a A0 paper sheet, can be recognized as a concrete object. This is just once relevant function of the vision part in this project; you can find more on the `vision_utilis.py` file.\n",
    "\n",
    "[cv2]: https://github.com/opencv/opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c61f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Gives a binary grid representation of the labyrinth (img must be grayscale)\n",
    "# ----\n",
    " \n",
    "def detect_labyrinth(img):\n",
    "  h,w = img.shape\n",
    "\n",
    "  # Remove AruCo\n",
    "  detected = detect_aruco(img)\n",
    "  erase_aruco(img, detected)\n",
    "\n",
    "  # Threshold definition\n",
    "  th = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV,31,2)\n",
    "\n",
    "  # Remove noise\n",
    "  kernel = np.ones((5,5),np.uint8)\n",
    "  th = cv.morphologyEx(th, cv.MORPH_OPEN, kernel)\n",
    "\n",
    "  # Detect connected components\n",
    "  num_labels, labels, stats, _ = cv.connectedComponentsWithStats(th, 8, cv.CV_32S)\n",
    "\n",
    "  # Pick wall components\n",
    "  result = np.zeros_like(th)\n",
    "  for i in range(1, num_labels):\n",
    "    if stats[i, cv.CC_STAT_AREA] > WALL_THRESHOLD:\n",
    "      result[labels == i] = 255\n",
    "\n",
    "  # Dilate along both x and y direction\n",
    "  kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE,(70, 70))\n",
    "  result = cv.dilate(result,kernel,iterations = 1)\n",
    "  result = dilate_walls_max(result, [5, 5], [10, 10], [20, 20])\n",
    "\n",
    "  result = dilate_walls_max(result, [5, 5],[10, 0], [20, 20])\n",
    "  result = dilate_walls_max(result, [5, 5], [0, 10], [20, 20])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500661ed",
   "metadata": {},
   "source": [
    "The goal of the `detect_labyrinth` function is to take as the input the raw image of the camera, usually it's in grayscale. Then by a series of image processing, it outputs the detected labyrinth. \n",
    "\n",
    "The raw image is first passed through an adaptive threshold. This will detect the walls. The kernel size is given large enough so that the walls are detected in one piece. Only applying a threshold is not enough. There could be parts of the thymio or the printed grid which could also be detected.\n",
    "\n",
    "<img src=\"images/vison_adaptive_threshold.png\" style=\"width: 300px;\">\n",
    "\n",
    "The second operation is to apply a morphogical opening (with a kernal size of 5x5) to reduce the detected noise. The image below shows the result image. `connectedComponentsWithStats` is then used to retrieve the connected components (see code above). Only the components which have a threshold area are kept. At this point, the walls are detected but they can be still disconnected due to morphological opening or image noise.\n",
    "\n",
    "<img src=\"images/vision_morphological_opening.png\" style=\"width: 300px;\">\n",
    "\n",
    "\n",
    "The final operation is to dilate the walls. This simple trick is to ensure that the navigation module will trace a path enough far from the walls so that the Thymio doesn't collide. The walls are progressively dilated until a certain margin between the walls is reached.\n",
    "\n",
    "<img src=\"images/vision_dilated.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cc3ed",
   "metadata": {},
   "source": [
    "#### Thymio's position distortion correction\n",
    "\n",
    "Using the camera, the Thymio robot is located using the ArUco code put on top of it as mentionned before. The idea seemed clever at first because all the localization was leveraged by the OpenCV library. One issue \n",
    "that was only apparent after the fact was that the center of the Thymio robot or its position is on the ground,\n",
    "while the ArUco is slightly elevated. This had the consequence to distort the position due to perspective. This was\n",
    "particularly apparent when it was close the edge of the image.\n",
    "\n",
    "<img src=\"images/thymio_top.png\" style=\"width: 200px;\">\n",
    "\n",
    "The problem can be resolved using more or less complex workarounds. The solution chosen for this project is to\n",
    "simply calibrate the offset manually. The Thymio robot is placed on the four corners of the labyrinth, the offset\n",
    "is entered using the script `vision/calibrate_offset.py` available in the project folder. The\n",
    "offset is then interpolated using scipy's interpolator which allows to get the offset for any position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_z_offset_data(path):\n",
    "  global offset_interp\n",
    "\n",
    "  f = open(path, \"rb\")\n",
    "  saved = pickle.load(f)\n",
    "\n",
    "  (thymio_pos, offset_pos) = saved\n",
    "\n",
    "  offset_interp = LinearNDInterpolator(thymio_pos, offset_pos, 0)\n",
    "\n",
    "\n",
    "def get_z_offset(center):\n",
    "  if offset_interp is not None:\n",
    "    return offset_interp(center[0], center[1])\n",
    "  else:\n",
    "    print(\"No interpolator for z offset (please use load_z_offset_data() first)\")\n",
    "    return [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335f748",
   "metadata": {},
   "source": [
    "# IV. Global navigation\n",
    "<a class=\"anchor\" id=\"Global-navigation\"></a>\n",
    "As our Thyminotaur is lazy and wants to reach the closest exit to the Greek beach, our global navigation provides him with the shortest path to follow. Furthermore Thyminotaur is rapidly sick when travelling, even more when turning in circles. Therefore, our global path is penalized for computing a route turning too much.\n",
    "\n",
    "Our global navigation implementation is based on the <i>motion cost approach</i>, where the cost corresponds to the distance from the origin to the next point. Besides the nominal cost, when turning to follow another trajectory instead of going straight, a fixed cost is added to the basic move.\n",
    "\n",
    "We use the A* algorithm to compute the path. Since our labyrinth map is made of squared cells and right-angled corner, the Dijkstra algorithm suits perfectly for this application. Extending it with a heuristic cost function can be an efficient way to optimize the search; however, we will see later on the complication caused by multiple exits.\n",
    "\n",
    "Finally, the global path given to the motion control module comprises a succession of absolute points on the XY Cartesian plane. This is the most convenient way to navigate our Thymio inside the rectangle, straight design maze. You can see an example of the camera / live implementation below.\n",
    "\n",
    "<img src=\"images/Path_planning.PNG\" style=\"width: 900px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b38f24",
   "metadata": {},
   "source": [
    "## Task summary\n",
    "\n",
    "The algorithm to compute the optimal path for the Thyminotaur can be described with the 7 following steps:\n",
    "\n",
    "1. Given the labyrinth map from the vision module, define the starting and the exits(s) positions.\n",
    "2. Resize the map for a faster computation.\n",
    "3. Create all possible nodes.\n",
    "4. Define the heuristic cost for each node.\n",
    "5. Find the optimal path, if such a path exists.\n",
    "6. Reconstruct the path with only the necessary nodes (when the direction change).\n",
    "7. Scale up the path to its original size.\n",
    "\n",
    "Within the following paragraphs, we will tackle the implementation of the A* algorithm, with the heuristic cost function and motion cost function. You can find more informations on the gloabl navigation module in the file `navigation\\nav_global_utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ed29d",
   "metadata": {},
   "source": [
    "### A* algorithm\n",
    "\n",
    "Our algorithm is based on the <i>laboratory n°5 - path planning</i> exercise, with slight changes to the two cost functions. \n",
    "\n",
    "Starting from the basis, the A* algorithm runs on the following cost function:\n",
    "\n",
    "$$ f(n) = g(n) + h(n) $$\n",
    "Where: \n",
    "- $g(n)$ is the motion cost.\n",
    "- $h(n)$ is the heuristic cost.\n",
    "- $f(n)$ is the total value of the node.\n",
    "\n",
    "The algorithm will then search the goal by exploring the neighborhood of the lowest node value $f(n)$. \n",
    "\n",
    "#### Heuristic cost function\n",
    "One heuristic cost function could attribute a cost to each node based on the flight distance (ignoring the walls) between the goal and the node. With this approach, the algorithm will be optimized to search the goal in direction of it, and therefore not searching in every direction. However, when we have multiple goals, how does the computation works? With a sum of each heuristic cost from each goal? This is clearly not a good option; with two goals close to each other, a strong penalization is computed for a third goal far away, and it will not be discovered even if this third goal is close to the starting point.\n",
    "\n",
    "\n",
    "We overcome this difficulty by turning the Thymio over the aim. Instead of searching from the starting point to the goals, we search now from the multiples goals towards the start. However, this is not as optimal as the unique goal case; if the two goals are far from each other, the entire map is discovered in the process.\n",
    "\n",
    "#### Motion cost function\n",
    "Regarding the motion cost, we add a feature which penalizes the changes of direction. We do this for two reasons:\n",
    "1. To avoid a moving in \"stair-step\", so the robot doesn't turn for every step.\n",
    "2. To improve odometry. Since the robot is more subject to prediction errors in position while turning, we can minimize these erros by moving straight.\n",
    "\n",
    "\n",
    "We show the `motion_cost` function below. It takes the previous, current and next position as parameters, in order to compute the last predicted movement. The <i>cost parameter</i> is a tuple of three values; the first and second values are normal cost in term of distance, while the third one is specifically the cost for the change of direction. If the change of direction cost is null, there are obviously no changes to consider. However, as we increase this cost, the path to the goal becomes longer but with fewer turns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd2bf8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:32:15.901365Z",
     "start_time": "2021-12-08T23:32:15.877428Z"
    }
   },
   "outputs": [],
   "source": [
    "def motion_cost(previous, current, next, defined_cost):\n",
    "    straight_cost = defined_cost[0]\n",
    "    diag_cost = defined_cost[1]\n",
    "    turn_cost = defined_cost[2]\n",
    "\n",
    "    prev_motion = np.asarray(current) - np.asarray(previous)\n",
    "    current_motion = np.asarray(next) - np.asarray(current)\n",
    "\n",
    "    if current_motion.all():\n",
    "        cost = diag_cost\n",
    "    else:\n",
    "        cost = straight_cost\n",
    "    # add cost for turning\n",
    "    if not(np.array_equal(prev_motion, current_motion)) and not(np.array_equal(prev_motion, [0,0])):\n",
    "        cost = cost + turn_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104fd11",
   "metadata": {},
   "source": [
    "The figures hereunder show some exemple of a computed path with three different additional cost for turning: 0, 1 and 5. As we can see, when we do not add penalities for turning, the motion is very staircase like. When the cost increase just a litle, the path is only made of long straight lines. Finally, when the cost is even bigger, the path change to a longer one, but with fewer turns. <font color='red'>TO COMPLETE: FIGURES</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997e9dc",
   "metadata": {},
   "source": [
    "# V. Local Navigation\n",
    "<a class=\"anchor\" id=\"Local-navigation\"></a>\n",
    "The local navigation runs continually while the Thymio is moving. This section is only implemented to avoid unexpected obstacles along the way, not to reorientate the Thymio after such an avoidance. Therefore, both motors are controlled by the `obstacle_avoidance_full` routine below, over the global navigation algorithm. \n",
    "\n",
    "This function is straightforward. It comprises different weights (`diffDelta`) to each of the 5 IR proximity sensors on Thymio's front. These weights give stronger importance to internal sensors over the exterior ones; this is because the Thymio needs to turn more when an obstacle is in front of him than on the side. Using a variable threshold (which can be tuned to match environment conditions), the Thymio can detect if an unexpected obstacle stands on his way to the exit:\n",
    "- When no obstacle is in sight, the robot is monitored by the global path navigation and goal tracking algorithm.\n",
    "- When something's on the way, `diffDelta` increases the speed of the motor close to the obstacle side; the closer the obstacle, the faster the motor.\n",
    "\n",
    "\n",
    "<img src=\"images/gif_thymio_avoidance.gif\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323197e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacle_avoidance_full(node, variables):\n",
    "    # global motor_left, motor_right\n",
    "    try:   \n",
    "        speed0 = 100       # nominal speed\n",
    "        speedGain = 5      # gain used with ground gradient\n",
    "        obstacleGain = 2   # gain used to avoid obstacle\n",
    "        obstThr = 20       # obstacle threshold to switch state 1->0\n",
    "        state = 0          # 0=gradient, 1=obstacle avoidance\n",
    "        prox = variables[\"prox.horizontal\"]\n",
    "\n",
    "        # difference between sensors\n",
    "        diffDelta = prox[0]+0.5*prox[1]-0.25*prox[2]-0.5*prox[3]-prox[4]\n",
    "\n",
    "        if state == 0 and (prox[0] > obstThr or prox[1] > obstThr or prox[2] > obstThr or prox[3] > obstThr or prox[4] > obstThr):\n",
    "        # switch from goal tracking to obst avoidance if obstacle detected\n",
    "           state = 1\n",
    "\n",
    "        elif state == 1 and (prox[0] < obstThr and prox[1] < obstThr and prox[2] < obstThr and prox[3] < obstThr and prox[4] < obstThr):\n",
    "        # switch from obst avoidance to goal tracking if obstacle got unseen\n",
    "             state = 0\n",
    "\n",
    "             if state == 0 :\n",
    "             # goal tracking: turn toward the goal\n",
    "                motor_left = speed0 - speedGain * diffDelta\n",
    "                motor_right = speed0 + speedGain * diffDelta           \n",
    "                \n",
    "             else :\n",
    "             # obstacle avoidance: accelerate wheel near obstacle\n",
    "                motor_left = speed0 + obstacleGain* diffDelta\n",
    "                motor_right = speed0 + obstacleGain* (-diffDelta)\n",
    "\n",
    "        node.send_set_variables(motors(motor_left, motor_right))\n",
    "    except KeyError:\n",
    "        pass  # prox.horizontal not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a67046",
   "metadata": {},
   "source": [
    "# VI. Filtering\n",
    "<a class=\"anchor\" id=\"Filtering\"></a>\n",
    "We choose to tackle the filtering part of this project by implementing an Extended Kalman Filter. This additional function is very useful when the signal between the camera and ArUcodes in corners is lost. As the global path stays on the computer, our Thymio has to wait for a signal every time he reaches a point on the path. Because of the poor light conditions, the ArUcodes may be detected periodically (\"blinking\" on our screen), leading to a jerking behaviour. The Extended Kalman Filter can solve this issue.\n",
    "\n",
    "The developed Kalman class can be found int the file `filter\\kalman.py` and will be explained in this section. The Kalman filter algorithm algorithm can be summarized in few steps (source: https://en.wikipedia.org/wiki/Extended_Kalman_filter):\n",
    "1. Prediction:\n",
    "    1. Predicted state estimate \n",
    "    2. Predicted covariance estimate\n",
    "2. Update:\n",
    "    1. Innovation or measurement residual\n",
    "    2. Innovation (or residual) covariance\n",
    "    3. Near-optimal Kalman gain\n",
    "    4. Updated state estimate\n",
    "    5. Updated covariance estimate \n",
    "    \n",
    "All these steps can be computed using the Kalman class function `kalmanEKF.filter(dt, Zs, Zc)` which take as parameters the time between two calls, the measurement matrices from the speed sensors ($Z_s$) and the camera ($Z_c$). We will see in the following subsections the implementation of the prediction and update steps, as well as the initializazion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c89193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(self, dt, Zs, Zc = None):\n",
    "    Zs = np.asarray(Zs).reshape((NB_SPEED_STATES,1))\n",
    "    if Zc is not None:\n",
    "        Zc = np.asarray(Zc).reshape((NB_CAM_STATES, 1))\n",
    "        if Zc[IDX_THETA] < 0: Zc[IDX_THETA] += 2*np.pi\n",
    "    self.dt = dt\n",
    "\n",
    "    self.predict()\n",
    "    self.update(Zs, Zc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536263e",
   "metadata": {},
   "source": [
    "## Prediction step\n",
    "First, the states of the robot had to be chosen. Since our robot is mostly moving at constant speed in a straight line, we chose the following states, without any acceleration states:\n",
    "\n",
    "$$X = \n",
    "\\begin{bmatrix}\n",
    "p_x\\\\\n",
    "p_y\\\\\n",
    "\\theta\\\\\n",
    "v\\\\\n",
    "\\omega\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where:\n",
    "- $p_x$, $p_y$ is the cartesian position in $[pxl]$.\n",
    "- $\\theta$ is the orientation in $[rad]$.\n",
    "- $v$ is the magnitude speed in $[pxl/s]$.\n",
    "- $\\omega$ is the angular speed $[rad/s]$.\n",
    "\n",
    "Because the motion control works in pixels and radian, we chose to keep these units for the states. \n",
    "\n",
    "For the model, given that the Thymio has a differential drive kinematics, the motion of the robot can be resumed to rotating around the instantaneous center of curvature (source: http://www.cs.columbia.edu/~allen/F17/NOTES/icckinematics.pdf). Therefore the transition functions used to predict future states are:\n",
    "\n",
    "$$\\label{eq.px}\n",
    "p_{x_{k+1}} = p_x + \\frac{v}{\\omega}(sin(\\theta+\\omega dt)-sin(\\theta))\n",
    "$$\n",
    "$$\\label{eq.py}\n",
    "p_{y_{k+1}} = p_y - \\frac{v}{\\omega}(cos(\\theta+\\omega dt)-cos(\\theta))\n",
    "$$\n",
    "$$\n",
    "\\theta_{k+1} = \\theta +\\omega dt\n",
    "$$\n",
    "$$\n",
    "v_{k+1} = v\n",
    "$$\n",
    "$$\n",
    "\\omega_{k+1} = \\omega\n",
    "$$\n",
    "\n",
    "Through the Kalman class, we can executed the prediction calling the function `kalmanEKF.predictX()`, where we added a a condition to avoid dividing by zero when the robot is going straight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b6619d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T23:56:47.327504Z",
     "start_time": "2021-12-11T23:56:47.275642Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_X(self):\n",
    "    # Prediction of the states, model approximation: v and w are constant\n",
    "    x = self.X[IDX_PX]\n",
    "    y = self.X[IDX_PY]\n",
    "    theta = self.X[IDX_THETA]\n",
    "    v = self.X[IDX_V]\n",
    "    w = self.X[IDX_W]\n",
    "    dt = self.dt\n",
    "\n",
    "    if np.abs(w)<=ALMOST_ZERO: # Driving straight, theta is constant\n",
    "        self.X[IDX_PX] = x + v * np.cos(theta) * dt\n",
    "        self.X[IDX_PY] = y + v * np.sin(theta) * dt\n",
    "        self.X[IDX_W] = ALMOST_ZERO\n",
    "    else: # otherwise\n",
    "        self.X[IDX_PX] = x + (v/w) * (np.sin(theta + w*dt) - np.sin(theta))\n",
    "        self.X[IDX_PY] = y - (v/w) * (np.cos(theta + w*dt) - np.cos(theta))\n",
    "        self.X[IDX_THETA] = (theta + w*dt) % (2.0*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ae820",
   "metadata": {},
   "source": [
    "As we can see on previous equations, the model is non linear and we thus have to use an extended Kalman filter to compute predicted covariance. Therefore we first have to evaluate the jacobian matrix of the transition function $F$, evalute at the current state. This is done by calling the Kalman class function `kalmanEKF.evaluateF()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4463152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T00:35:02.596018Z",
     "start_time": "2021-12-12T00:35:02.572024Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_F(self):\n",
    "    # Calculation of the Jacobian of our non linear state prediction system f\n",
    "    x = self.X[IDX_PX]\n",
    "    y = self.X[IDX_PY]\n",
    "    theta = self.X[IDX_THETA]\n",
    "    v = self.X[IDX_V]\n",
    "    w = self.X[IDX_W]\n",
    "    dt = self.dt\n",
    "\n",
    "    f13 = (v/w) * (np.cos(theta + w*dt) - np.cos(theta))\n",
    "    f14 = (1.0/w) * (np.sin(theta + w*dt) - np.sin(theta))\n",
    "    f15 = (v*dt/w) * np.cos(theta + w*dt) - (v/w**2) * (np.sin(theta + w*dt) - np.sin(theta))\n",
    "    f23 = (v/w) * (np.sin(theta + w*dt) - np.sin(theta))\n",
    "    f24 = (-1.0/w) * (np.cos(theta + w*dt) - np.cos(theta))\n",
    "    f25 = (v*dt/w) * np.sin(theta + w*dt) + (v/w**2) * (np.cos(theta + w*dt) - np.cos(theta))\n",
    "    self.F = np.array([[1.0, 0.0, f13, f14, f15],\n",
    "                       [0.0, 1.0, f23, f24, f25],\n",
    "                       [0.0, 0.0, 1.0, 0.0, dt],\n",
    "                       [0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "                       [0.0, 0.0, 0.0, 0.0, 1.0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608df6dc",
   "metadata": {},
   "source": [
    "Finally, we can compute the predicted states and predicted covariance by simply calling the function `kalamanEKF.predict()`, where $Q$ is the process noise which will be explained in the section \"Initialization step\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986dd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self):\n",
    "    self.evaluate_F()\n",
    "    self.P = self.F @ self.P @ self.F.T + self.Q\n",
    "    self.predict_X()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb51491",
   "metadata": {},
   "source": [
    "## Update step\n",
    "\n",
    "The update of the estimated states are done by using two sensors. The primary one ($Z_c$) is the camera given us directly the states $p_x$, $p_y$ and $\\theta$. The secondary sensors are the one measuring the speed of each wheel ($v_r$, $v_l$) of the Thimyo. From these measurement we can obtain the states variable $v$ and $\\omega$ as follow:\n",
    "$$\n",
    "v = \\frac{v_r + v_l}{2}\n",
    "$$\n",
    "$$\n",
    "\\omega = \\frac{v_r - v_l}{L}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- L is the distance between the wheels.\n",
    "\n",
    "With these two sensors, we can compute the observation $H$ matrix, to compute the innovation or measurement residual:\n",
    "$$ H_c =  \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "p_x\\\\\n",
    "p_y\\\\\n",
    "\\theta\\\\\n",
    "v\\\\\n",
    "\\omega\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_s = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & \\frac{L}{2} \\\\\n",
    "0 & 0 & 0 & 1 & -\\frac{L}{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "p_x\\\\\n",
    "p_y\\\\\n",
    "\\theta\\\\\n",
    "v\\\\\n",
    "\\omega\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In addition to the $H_s$ matrix, we have to convert the speed states [pxl/s] into Thymio's units. This is done by a conversion factor determined experimentaly: we measured the distance in pixels travelled by the Thymio at defined speed for a defined time.\n",
    "\n",
    "Since our observation matrices are linear, it is equal to its the jacobian matrices and therefore we can directly use them to compute the innovation covariance matrices and optimal gain. Then we can update the predicted states and predicted covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, Zs, Zc):\n",
    "    # update with speed measurement\n",
    "    Y = Zs - self.Hs @ self.X\n",
    "    S = self.Hs @ self.P @ self.Hs.T + self.Rs\n",
    "    K = self.P @ self.Hs.T @ np.linalg.inv(S)\n",
    "    self.X = self.X + K @ Y\n",
    "    self.P = (np.eye(NB_STATES)- K @ self.Hs) @ self.P\n",
    "\n",
    "    # update with camera measurement\n",
    "    if Zc is not None:\n",
    "        Y = Zc - self.Hc @ self.X\n",
    "        S = self.Hc @ self.P @ self.Hc.T + self.Rc\n",
    "        K = self.P @ self.Hc.T @ np.linalg.inv(S)\n",
    "        self.P = (np.eye(NB_STATES) - K @ self.Hc) @ self.P\n",
    "        self.X = self.X + K @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6ee3d",
   "metadata": {},
   "source": [
    "## Initialization step\n",
    "\n",
    "When we instance an object of class `kalmanEKF` it creates the different matrices with their predifined size. In addition, it creates the initial parameters of the initial state, initial covariance state, process noise $Q$, camera measurement noise $R_c$ and speed measurement noise $R_s$. Some helper function are defined at the end of the class to initialize the different matrices.\n",
    "\n",
    "### Initial state and covariance\n",
    "As can not know the starting position and orientation before measuring it with the camera, we set the initial states at zero, with a high covariance:\n",
    "\n",
    "$$X_0 = \n",
    "\\begin{bmatrix}\n",
    "0\\\\0\\\\0\\\\0\\\\0\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$P_0 =\n",
    "\\begin{bmatrix}\n",
    "1000&0&0&0&0\\\\\n",
    "0&1000&0&0&0\\\\\n",
    "0&0&1000&0&0\\\\\n",
    "0&0&0&1000&0\\\\\n",
    "0&0&0&0&1000\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Process noise $Q$\n",
    "The model used is simplified and do not take into account many paramaters, which we can correct a little by placing noise on their states. For example, the model is based on constant speed speed, even though the robot is turning. Therefore the noise on $v$ state and $\\omega$ state are quite high. We can make a non exhausitve list of variables making the model less accurate:\n",
    "- Model with constant speed\n",
    "- Placement of the arUcode on the Thymio not centered.\n",
    "- Friction of the wheels\n",
    "- Electrical noise on the command\n",
    "- Dimension of the robot\n",
    "To simplify the estimation, we assumed that the process noise matrix is diagonal and therefore it has no correlation between the states even though we could have off-diagonal terms. After tuning the parameters, we ended-up the following matrix:\n",
    "\n",
    "$$Q=\n",
    "\\begin{bmatrix}\n",
    "10&0&0&0&0\\\\\n",
    "0&10&0&0&0\\\\\n",
    "0&0&0.3&0&0\\\\\n",
    "0&0&0&20&0\\\\\n",
    "0&0&0&0&3\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Camera measurement noise $R_c$\n",
    "We assumed that there are no correlation between the position and the orientation, so the matrix is a diagonal one. Moreover, it should not evolve over time so the matrix is static and defined only once during the initialization To estimate the variance, we measured several times the static robot position and orientation. We then compute the variance of these data, which are:\n",
    "$$R_c = \n",
    "\\begin{bmatrix}\n",
    "\\sigma_{p_x}^2 & 0 & 0 \\\\\n",
    "0 & \\sigma_{p_y} ^2 & 0 \\\\\n",
    "0 & 0 & \\sigma_{\\theta}^2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.40 & 0 & 0 \\\\\n",
    "0 & 0.03 & 0 \\\\\n",
    "0 & 0 & 0.01 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Speed measurement noise $R_s$\n",
    "Again, we assumed that there are no correlation between the right speed and left speed of the wheels, so the matrix is a diagonal one, as well that there is no dependencies over time. To estimate the variance, we measured several times the speed of the wheels while they were given a constant command. We then compute the variance of these data, which are:\n",
    "$$R_c = \n",
    "\\begin{bmatrix}\n",
    "\\sigma_{v_r}^2 & 0 \\\\\n",
    "0 & \\sigma_{v_l} ^2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6.32 & 0 \\\\\n",
    "0 & 3.89 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Results\n",
    "The implemented Kalman filter is benefic to the system. Indeed, when the camera fails to localize the Thymio, the position and orientation is still estimate through the speed sensors and model of the robot. We therefore can correctly move up to two turns and around four cells before stepping on the walls. As soon as the camera is avalaible, the position and orientation are fixed with a more precise. <font color='red'>TO COMPLETE: FIGURES? VIDEOS? COMMENTS? RELECTURE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c984d1",
   "metadata": {},
   "source": [
    "# VII. Motion Control\n",
    "<a class=\"anchor\" id=\"Motion-control\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171a077",
   "metadata": {},
   "source": [
    "The motion control aims to provide the correct motors command to the robot, depending on its position ($P_{x_r}$, $P_{y_r}$, $\\alpha_r$) and the path to follow computed previously. The control approach is direct, because the trajectories can only be composed of straight lines in the actual maze. Therefore, the robot can accurately follow the path simply by moving to the intersection points of the straight lines generated by the Dijkstra’s algorithm.\n",
    "The motors command can be decomposed into three terms :\n",
    "\n",
    "$$\\vec v_{mot} = \\vec v_{orientation} + \\vec v_{position} + \\vec v_{obstacle}$$\n",
    "\n",
    "1. **Orientation**: When the next point $P_{k+1}$ where the robot needs to go is known, the robot has to correct its orientation $\\alpha_r$. A simple P regulator is used to minimize the error $\\alpha_e = \\alpha_r - \\alpha_c$, where $\\alpha_c$ is the commanded orientation of the robot. The following equations show how the angular command is computed and then how is deduced the motor speed (orientation part).\n",
    "\n",
    "$$\\alpha_c = - atan2(\\frac{Py_{k+1} - P_{y_r}}{Px_{k+1} - P_{x_r}})$$\n",
    "\n",
    "$$\\vec v_{orientation} = \\pm K_{p_\\alpha} \\cdot \\alpha_e$$\n",
    "    \n",
    "  The $\\pm$ sign is used to notify that the robot needs to turn, so the speed of the left and the right wheel need to be in  the opposite directions.\n",
    "\n",
    "\n",
    "2. **Position**: Now that the robot has the right orientation, it needs to move forward to reach the point $P_{k+1}$. The motor command for this part is not computed using a classic P control law because it is based on the angular error. The aim is to have a constant speed when the robot is travelling between two points with the right orientation but also to have a speed close to zero (for this part) when the orientation is wrong : \n",
    "\n",
    "$$\\vec v_{position} = K_{p_d} \\cdot (180 - \\mid\\alpha_e\\mid)$$\n",
    "\n",
    "<img src=\"images/MotionControl_scheme_net.png\" style=\"width: 600px;\">\n",
    "\n",
    "3. **Obstacle avoidance**: This part also sets a speed command using rules that are already described in its own section.\n",
    "\n",
    "These three parts are the main ideas of the motion control approach used for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195ec39",
   "metadata": {},
   "source": [
    "## Implementation problematics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267f3a6",
   "metadata": {},
   "source": [
    "This motion control implementation involved being careful about the following points :\n",
    "   - **Referential consideration** : Two angles need to be considered for this problem, the orientation of the robot $\\alpha_r$ and the angle between the vector $\\vec d(t)$ and the $\\vec y$ axis, which is used to set the orientation command of the robot. This was more difficult than expected to ensure consistence between these two values in any situation.\n",
    "   \n",
    "   \n",
    "   - **Angular discontinuity** : Because an angle is defined between $0 ~rad$ and $2\\pi ~rad$ it creates a huge discontinuity when the robot angle is close to $2\\pi ~rad$ and suddenly goes to $0 ~rad$. For example, suppose the robot needs to reach an orientation of $0 rad$ (which is the discontinuity location) and has a current orientation of $\\pi/6 ~rad$, then the error is $\\alpha_e = \\pi/6 - 0 = 0 ~rad$. The generated motors command will reduce the error to reach $\\alpha_e = 0 ~ rad$ so $\\alpha_r = 0 ~rad$. But if the robot goes further (overshoot) then its orientation will jump from approx. 0 rad to approx. $2\\pi ~rad$. Therefore, the error becomes $\\alpha_e = 2\\pi - 0 = 2\\pi ~rad$ and so the robot will continue to turn on itself. This effect has been avoided by limiting the orientation error $\\alpha_e$ between $-\\pi$ and $\\pi$. For example, if the computed error is $3\\pi/2~rad$ then it is changed to be equal to $-\\pi/2~rad$ which is equivalent.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244a1c8",
   "metadata": {},
   "source": [
    "## Improvement of the motion control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2fd4e",
   "metadata": {},
   "source": [
    "After several trials, we have identified the following problems:\n",
    "   - **Overshoot after reaching a point** : Because of the command law presented previously, the speed of the robot doesn't decrease when it arrives close to its point to reach ($P_{k+1}$). So in this configuration, there is a trade-off between the speed of the robot between two points and the overshoot. To solve this issue, the control law has been improved by modifying the gains $K_{P_\\alpha}$ and $K_{P_d}$ depending if the robot is close to the point to reach or not. If the robot is close (distance <= 20 pixels) to $P_{k+1}$ the gains are $K_{P_d} = 0.5$ and $K_{P_\\alpha} = 4$. If the robot stands further (distance > 20 pixels) to $P_{k+1}$ then the gains are $K_{P_d} = 1$ and $K_{P_\\alpha} = 1$. In this configuration, the robot can easily turn without overshoot when coming close to $P_{k+1}$, because the weight of the orientation command increase relatively to the weight of the position command. It can also move faster when it is far from $P_{k+1}$.\n",
    "\n",
    "\n",
    "   \n",
    "   - **Conflict between $\\vec v_{position}$ and $\\vec v_{orientation}$** : This problem is like the previous one. It first appeared when the robot started its run very close (approximately 10 pixels) to the next point to reach $P_{k+1}$, oriented in the opposite direction of this latter. In this configuration, the robot could not reach the point because the linear speed command was too high in comparison with the rotation command speed. In fact, the robot could only turn around the point. This problem has also been corrected, thanks to the previously shown a modification of the control law, which gives more weight to the orientation command when the robot is close (distance < 20 pixels) to $P_{k+1}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab0254",
   "metadata": {},
   "source": [
    "# VIII . Main\n",
    "<a class=\"anchor\" id=\"Main\"></a>\n",
    "If you desire to execute the code from the notebook, the following cells must be run.\n",
    "1. Instalation of the required packages\n",
    "2. Cloning the github repository\n",
    "3. Modifying the path \n",
    "4. Running the main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instal the required packages, p.ex:\n",
    "# !pip install opencv-python tqdm matplotlib numpy ipywidgets python-dotenv pyserial\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Thyminotaur/labyrinth-project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd labyrinth-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96248cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
