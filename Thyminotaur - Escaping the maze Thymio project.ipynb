{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7692fcd",
   "metadata": {},
   "source": [
    "# [MICRO-452:][MICRO-452] Project Report\n",
    "\n",
    "\n",
    "\n",
    "<p><b>Authors:</b> &nbsp;&emsp;&emsp;&emsp;&emsp;&emsp;Stephen Monnet, David Rüegg, Julien Burkhard, Sylvain Jacquart<br>\n",
    "<b>Supervisors:</b> &nbsp;&emsp;&emsp;&emsp;Prof. Francesco Mondada, Laila El-Hamamsy<br>\n",
    "<b>Due date:</b>  &nbsp;&nbsp;&nbsp;&emsp;&emsp;&emsp;&emsp;December 12th, 2021</p>\n",
    "<b>Presentation date:</b> &nbsp;&nbsp;December 16th, 2021</p>\n",
    "\n",
    "[MICRO-452]: https://moodle.epfl.ch/pluginfile.php/2727652/mod_resource/content/3/Week%209%20-%20Project%20Presentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a39a58",
   "metadata": {},
   "source": [
    "<h1><center> Thyminotaur - εσκαπινγ θε μαζε (Εscaping the maze) </center></h1>\n",
    "<img src=\"images/angry_thymio.gif\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3623776",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1><br>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Project-summary-,-hardware-and-choices\" data-toc-modified-id=\"Project-summary-,-hardware-and-choices-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Project summary, hardware and choices</a></li></ul><ul class=\"toc-item\"><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Motion-control\" data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Main\" data-toc-modified-id=\"Main-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Main</a></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2aea18",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "<a class=\"anchor\" id=\"Introduction\"></a>\n",
    "<p style='text-align: justify;'> \n",
    "    This mobile robotics project is inspired by the old legend of <i>Theseus and the Labyrinth</i>. It all started during the SHS course <i>Myths of the ancient Mediterranean Sea</i> with the following question:</p>\n",
    "\n",
    "**What if the Minotaur, trapped forever in the vast Labyrinth built on Creta island, could find his way out and live his best life under the greek sun?**\n",
    "\n",
    "The major goal of this project is therefore to help the little Thyminotaur to find his way out of the maze! To help him with his quest for freedom, the 5 following tools are implemented in the project.  \n",
    "\n",
    "1. **Vision**: Placed above the maze, a camera detects the walls, maze exits and the position of the robot. The optimal path is then computed with the vision information. We also perform calibration prior to the execution of the program, to guarantee an accurate tracking of the Thymio.\n",
    "\n",
    "\n",
    "2. **Global Navigation**: With the help of the camera, the global path to the nearest exit of the labyrinth is compiled and send as the route to follow for the Thymio. The nearest exit is detected with a common cost-function algorithm.\n",
    "\n",
    "\n",
    "3. **Local Navigation**: With the help of the 5 front infrared sensors, the Thymio can avoid unexpected obstacles along the way. We assign a different weight to each sensor, to ensure a smooth obstacle avoidance behaviour a minimum of wall-crossing during these manoeuvres.\n",
    "\n",
    "\n",
    "4. **Filtering**: An extended Kalman Filter is implemented in this project, as a support to the vision part. Using this filter, we can track the robot with a precision in linear speed/position of XX and in angular speed/angle of YY. <font color='red'>TO COMPLETE</font>\n",
    "\n",
    "\n",
    "5. **Motion Control**: Once the path is computed, the information is transmitted to the Thymio which starts moving.\n",
    "\n",
    "\n",
    "The main folder of the project can be found following the repository: https://github.com/jbyuki/labyrinth-project\n",
    "\n",
    "The global decision tree for the project can be summarized in the figure below. Our Thyminotaur is robust to a camera blinding (thanks to the Kalman filter) but not to kidnapping; after all, it is hard to kidnap robots surrounded by labyrinth walls...\n",
    "\n",
    "<img src=\"images/Decision_tree.PNG\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142ac74",
   "metadata": {},
   "source": [
    "# II. Project summary, hardware and choices\n",
    "<a class=\"anchor\" id=\"Project-summary-,-hardware-and-choices\"></a>\n",
    "### Project summary\n",
    "\n",
    "<img src=\"images/GlobalExplenation.PNG\" style=\"width: 600px;\">\n",
    "\n",
    "1. We place the Thymio on a random cell of the labyrinth. The camera detects the *ArUco Marker* on the Thymio, compiling its position and orientation. \n",
    "\n",
    "\n",
    "2. The labyrinth is drawn with 2 exits; using the Dijkstra’s algorithm, the Thymio follows the path to the nearest exit of the maze (green path on the figure).\n",
    "\n",
    "\n",
    "3. During the path following, unexpected obstacles will block the road to our adventurous Thyminotaur. Thanks to his local avoidance abilities (based on his proximity sensors/combination of weights), no unexpected event will stand between him and the exit. He bypasses every obstacle until the maze exit.\n",
    "\n",
    "\n",
    "4. The Thymio stops at the exit of the Labyrinth, satisfied with his regained freedom.\n",
    "\n",
    "### Hardware\n",
    "<p style='text-align: justify;'>\n",
    "The labyrinth covers a 4x5 cells unit (17cm x 20cm each) and is printed on an A0 white paper (shout-out to the PolyRepro for their fast delivery). To ensure a good camera tracking, the Thymio carries an <i> ArUco Marker</i>, which allows to detect the position and orientation of the robot. The marker is stuck to <i>Lego bricks</i> and mounted directly on top of the Thymio, as you can see in the picture below.</p>\n",
    "\n",
    "<img src=\"images/combined_pic_thymio_lab.PNG\" style=\"width: 600px; \">\n",
    "\n",
    "\n",
    "### Additional choices for this project\n",
    "1. At first, a solid labyrinth made of wood was considered enclosing the Thymio. After several tries and prototypes, the full paper option (black lines printed on a large white sheet) was selected for its practicality and easiness of implementation with the vision part.\n",
    "\n",
    "    \n",
    "2. To mark out the available space of the Thymio (his playground), 4 additional arUco Markers are placed on each corner of the sheet. The camera placed above the labyrinth is never perfectly aligned with the sheet, both in position and angle; the arUcodes, combined with extreme-positions calibration, allow a straightforward flattening of the image at the vision initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68597e1",
   "metadata": {},
   "source": [
    "# III. Vision\n",
    "<a class=\"anchor\" id=\"Vision\"></a>\n",
    "\n",
    "In the Thyminotaur project, the whole vision data is provided by a single static camera located over the robot's scope of operation. We use vision during the initialization phase (labyrinth recognition, full-scale calibration, corners detection) and while the robot is moving (Thymio detection). We coded the entire vision part using the Python library <i>Open Source Computer Vision</i>, or [open cv][cv2] in short.\n",
    "\n",
    "We assigned arUco Markers to critical variables of the project (the moving Thymio and the 4 corners) to ensure consistency in the code. These markers are a simple yet efficient way to convert information from a full vision scope to a code of variables and raw data (images on left & center). For corners, it is just a way to fix the operating frame of the robot. On the Thymio, the marker allows to track the position and angle of the Thyminotaur from the starting point in real time.\n",
    "\n",
    "The labyrinth is made of large, 20mm-width black lines printed of white paper to achieve the best contrast. This simplifies our program in defining the possible paths for the robot (image on right). Since system vision is a key element in both the global navigation algorithm and for general system motion control, we directly included the vision functions in each class rather than putting them in their own separate class. <br>\n",
    "\n",
    "<img src=\"images/vision_3_image.PNG\" style=\"width: 900px;\">\n",
    "    \n",
    "The `detect_labyrinth` function below summarizes how black lines, on a A0 paper sheet, can be recognized as a concrete object. This is just once relevant function of the vision part in this project; you can find more on the `vision_utilis.py` file. The `dilate` function is an excellent way to detect all possible paths in the labyrinth, centered in the middle of corridors.\n",
    "    \n",
    "[cv2]: https://github.com/opencv/opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Gives a binary grid representation of the labyrinth (img must be grayscale)\n",
    "# ----\n",
    " \n",
    "def detect_labyrinth(img):\n",
    "  h,w = img.shape\n",
    "\n",
    "  # Remove AruCo\n",
    "  detected = detect_aruco(img)\n",
    "  erase_aruco(img, detected)\n",
    "\n",
    "\n",
    "  # Threshold definition\n",
    "  th = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV,31,2)\n",
    "  \n",
    "  cv.imwrite(\"global_trajectory_real_resized_th.png\", th)\n",
    "\n",
    "  # Remove noise\n",
    "  kernel = np.ones((10,10),np.uint8)\n",
    "  kernel2 = np.ones((25,25),np.uint8)\n",
    "  th = cv.morphologyEx(th, cv.MORPH_OPEN, kernel)\n",
    "  # th = cv.dilate(th, kernel2, iterations=1)\n",
    "\n",
    "  # Detect connected components\n",
    "  num_labels, labels, stats, _ = cv.connectedComponentsWithStats(th, 8, cv.CV_32S)\n",
    "\n",
    "  # Pick wall components\n",
    "  result = np.zeros_like(th)\n",
    "  for i in range(1, num_labels):\n",
    "    if stats[i, cv.CC_STAT_AREA] > WALL_THRESHOLD:\n",
    "      result[labels == i] = 255\n",
    "\n",
    "  # temp code\n",
    "  # result = th\n",
    "  cv.imwrite(\"global_trajectory_real_resized_test.png\", th)\n",
    "\n",
    "  # Dilate along both x and y direction\n",
    "  kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE,(20, 20))\n",
    "  result = cv.dilate(result,kernel,iterations = 1)\n",
    "  result = dilate_walls_max(result, [5, 5], [10, 10], [20, 20])\n",
    "\n",
    "  result = dilate_walls_max(result, [5, 5],[10, 0], [20, 20])\n",
    "  result = dilate_walls_max(result, [5, 5], [0, 10], [20, 20])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335f748",
   "metadata": {},
   "source": [
    "# IV. Global navigation\n",
    "<a class=\"anchor\" id=\"Global-navigation\"></a>\n",
    "As our Thyminotaur is lazy and wants to reach the closest exit to the Greek beach, our global navigation provides him with the shortest path to follow. Furthermore Thyminotaur is rapidly sick when travelling, even more when turning in circles. Therefore, our global path is penalized for computing a route turning too much.\n",
    "\n",
    "Our global navigation implementation is based on the <i>motion cost approach</i>, where the cost corresponds to the distance from the origin to the next point. Besides the nominal cost, when turning to follow another trajectory instead of going straight, a fixed cost is added to the basic move.\n",
    "\n",
    "We use the A* algorithm to compute the path. Since our labyrinth map is made of squared cells and right-angled corner, the Dijkstra algorithm suits perfectly for this application. Extending it with a heuristic cost function can be an efficient way to optimize the search; however, we will see later on the complication caused by multiple exits.\n",
    "\n",
    "Finally, the global path given to the motion control module comprises a succession of absolute points on the XY Cartesian plane. This is the most convenient way to navigate our Thymio inside the rectangle, straight design maze. You can see an example of the camera / live implementation below.\n",
    "\n",
    "<img src=\"images/Path_planning.PNG\" style=\"width: 900px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b38f24",
   "metadata": {},
   "source": [
    "## Task summary\n",
    "\n",
    "The algorithm to compute the optimal path for the Thyminotaur can be described with the 7 following steps:\n",
    "\n",
    "1. Given the labyrinth map from the vision module, define the starting and the exits(s) positions.\n",
    "2. Resize the map for a faster computation.\n",
    "3. Create all possible nodes.\n",
    "4. Define the heuristic cost for each node.\n",
    "5. Find the optimal path, if such a path exists.\n",
    "6. Reconstruct the path with only the necessary nodes (when the direction change).\n",
    "7. Scale up the path to its original size.\n",
    "\n",
    "Within the following paragraphs, we will tackle the implementation of the A* algorithm, with the heuristic cost function and motion cost function. You can find more informations on the gloabl navigation module in the file `navigation\\nav_global_utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ed29d",
   "metadata": {},
   "source": [
    "### A* algorithm\n",
    "\n",
    "Our algorithm is based on the <i>laboratory n°5 - path planning</i> exercise, with slight changes to the two cost functions. \n",
    "\n",
    "Starting from the basis, the A* algorithm runs on the following cost function:\n",
    "\n",
    "$$ f(n) = g(n) + h(n) $$\n",
    "Where: \n",
    "- $g(n)$ is the motion cost.\n",
    "- $h(n)$ is the heuristic cost.\n",
    "- $f(n)$ is the total value of the node.\n",
    "The algorithm will then search the goal by exploring the neighborhood of the lowest node value. \n",
    "\n",
    "#### Heuristic cost function\n",
    "One heuristic cost function could attribute a cost to each node based on the flight distance (ignoring the walls) between the goal and the node. With this approach, the algorithm will be optimized to search the goal in direction of it, and therefore not searching in every direction. However, when we have multiple goals, how does the computation works? With a sum of each heuristic cost from each goal? This is clearly not a good option; with two goals close to each other, a strong penalization is computed for a third goal far away, and it will not be discovered even if this third goal is close to the starting point.\n",
    "\n",
    "\n",
    "We overcome this difficulty by turning the Thymio over the aim. Instead of searching from the starting point to the goals, we search now from the multiples goals towards the start. However, this is not as optimal as the unique goal case; if the two goals are far from each other, the entire map is discovered in the process.\n",
    "\n",
    "#### Motion cost function\n",
    "Regarding the motion cost, we add a feature which penalizes the changes of direction. We do this for two reasons:\n",
    "1. To avoid a moving in \"stair-step\", so the robot doesn't turn for every step.\n",
    "2. To improve odometry. Since the robot is more subject to prediction errors in position while turning, we can minimize these erros by moving straight.\n",
    "\n",
    "\n",
    "We show the `motion_cost` function below. It takes the previous, current and next position as parameters, in order to compute the last predicted movement. The <i>cost parameter</i> is a tuple of three values; the first and second values are normal cost in term of distance, while the third one is specifically the cost for the change of direction. If the change of direction cost is null, there are obviously no changes to consider. However, as we increase this cost, the path to the goal becomes longer but with fewer turns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd2bf8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:32:15.901365Z",
     "start_time": "2021-12-08T23:32:15.877428Z"
    }
   },
   "outputs": [],
   "source": [
    "def motion_cost(previous, current, next, defined_cost):\n",
    "    straight_cost = defined_cost[0]\n",
    "    diag_cost = defined_cost[1]\n",
    "    turn_cost = defined_cost[2]\n",
    "\n",
    "    prev_motion = np.asarray(current) - np.asarray(previous)\n",
    "    current_motion = np.asarray(next) - np.asarray(current)\n",
    "\n",
    "    if current_motion.all():\n",
    "        cost = diag_cost\n",
    "    else:\n",
    "        cost = straight_cost\n",
    "    # add cost for turning\n",
    "    if not(np.array_equal(prev_motion, current_motion)) and not(np.array_equal(prev_motion, [0,0])):\n",
    "        cost = cost + turn_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104fd11",
   "metadata": {},
   "source": [
    "## Exemple of global navigation\n",
    "\n",
    "Here we can see some exemple of the computation of the global path. First, we have the resizing of the map and the computed path. On the last figures, we can see how the cost function is taken into account. <font color='red'>TO COMPLETE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997e9dc",
   "metadata": {},
   "source": [
    "# V. Local Navigation\n",
    "<a class=\"anchor\" id=\"Local-navigation\"></a>\n",
    "The local navigation runs continually while the Thymio is moving. This section is only implemented to avoid unexpected obstacles along the way, not to reorientate the Thymio after such an avoidance. Therefore, both motors are controlled by the `obstacle_avoidance_full` routine below, over the global navigation algorithm. \n",
    "\n",
    "This function is straightforward. It comprises different weights (`diffDelta`) to each of the 5 IR proximity sensors on Thymio's front. These weights give stronger importance to internal sensors over the exterior ones; this is because the Thymio needs to turn more when an obstacle is in front of him than on the side. Using a variable threshold (which can be tuned to match environment conditions), the Thymio can detect if an unexpected obstacle stands on his way to the exit:\n",
    "- When no obstacle is in sight, the robot is monitored by the global path navigation and goal tracking algorithm.\n",
    "- When something's on the way, `diffDelta` increases the speed of the motor close to the obstacle side; the closer the obstacle, the faster the motor.\n",
    "\n",
    "\n",
    "<img src=\"images/gif_thymio_avoidance.gif\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323197e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacle_avoidance_full(node, variables):\n",
    "    # global motor_left, motor_right\n",
    "    try:   \n",
    "        speed0 = 100       # nominal speed\n",
    "        speedGain = 5      # gain used with ground gradient\n",
    "        obstacleGain = 2   # gain used to avoid obstacle\n",
    "        obstThr = 20       # obstacle threshold to switch state 1->0\n",
    "        state = 0          # 0=gradient, 1=obstacle avoidance\n",
    "        prox = variables[\"prox.horizontal\"]\n",
    "\n",
    "        # difference between sensors\n",
    "        diffDelta = prox[0]+0.5*prox[1]-0.25*prox[2]-0.5*prox[3]-prox[4]\n",
    "\n",
    "        if state == 0 and (prox[0] > obstThr or prox[1] > obstThr or prox[2] > obstThr or prox[3] > obstThr or prox[4] > obstThr):\n",
    "        # switch from goal tracking to obst avoidance if obstacle detected\n",
    "           state = 1\n",
    "\n",
    "        elif state == 1 and (prox[0] < obstThr and prox[1] < obstThr and prox[2] < obstThr and prox[3] < obstThr and prox[4] < obstThr):\n",
    "        # switch from obst avoidance to goal tracking if obstacle got unseen\n",
    "             state = 0\n",
    "\n",
    "             if state == 0 :\n",
    "             # goal tracking: turn toward the goal\n",
    "                motor_left = speed0 - speedGain * diffDelta\n",
    "                motor_right = speed0 + speedGain * diffDelta           \n",
    "                \n",
    "             else :\n",
    "             # obstacle avoidance: accelerate wheel near obstacle\n",
    "                motor_left = speed0 + obstacleGain* diffDelta\n",
    "                motor_right = speed0 + obstacleGain* (-diffDelta)\n",
    "\n",
    "        node.send_set_variables(motors(motor_left, motor_right))\n",
    "    except KeyError:\n",
    "        pass  # prox.horizontal not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a67046",
   "metadata": {},
   "source": [
    "# VI. Filtering\n",
    "<a class=\"anchor\" id=\"Filtering\"></a>\n",
    "We choose to tackle the filtering part of this project by implementing an Extended Kalman Filter. This additional function is very useful when the signal between the camera and ArUcodes in corners is lost. As the global path stays on the computer, our Thymio has to wait for a signal every time he reaches a point on the path. Because of the poor light conditions, the ArUcodes may be detected periodically (\"blinking\" on our screen), leading to a jerking behaviour. The Extended Kalman Filter can solve this issue.\n",
    "\n",
    "\n",
    "7.8/21\n",
    "estimer le probabilité\n",
    "distri de proba, à ajuster en continu. Quand il avance dans l'ombre (sans cam), plus la gaussienne s'applatit.\n",
    "Camera = sensor absolu, pas d'incertitude sur la position / sans cam = accéléromètre, vitesse -> sensor relatif\n",
    "Kalmann ->, général, Gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e99c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code space for the Kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c984d1",
   "metadata": {},
   "source": [
    "# VII. Motion Control\n",
    "<a class=\"anchor\" id=\"Motion-control\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171a077",
   "metadata": {},
   "source": [
    "The motion control aims to provide the correct motors command to the robot, depending on its position ($P_{x_r}$, $P_{y_r}$, $\\alpha_r$) and the path to follow computed previously. The control approach is direct, because the trajectories can only be composed of straight lines in the actual maze. Therefore, the robot can accurately follow the path simply by moving to the intersection points of the straight lines generated by the Dijkstra’s algorithm.\n",
    "The motors command can be decomposed into three terms :\n",
    "\n",
    "$$\\vec v_{mot} = \\vec v_{orientation} + \\vec v_{position} + \\vec v_{obstacle}$$\n",
    "\n",
    "1. **Orientation**: When the next point $P_{k+1}$ where the robot needs to go is known, the robot has to correct its orientation $\\alpha_r$. A simple P regulator is used to minimize the error $\\alpha_e = \\alpha_r - \\alpha_c$, where $\\alpha_c$ is the commanded orientation of the robot. The following equations show how the angular command is computed and then how is deduced the motor speed (orientation part).\n",
    "\n",
    "$$\\alpha_c = - atan2(\\frac{Py_{k+1} - P_{y_r}}{Px_{k+1} - P_{x_r}})$$\n",
    "\n",
    "$$\\vec v_{orientation} = \\pm K_{p_\\alpha} \\cdot \\alpha_e$$\n",
    "    \n",
    "  The $\\pm$ sign is used to notify that the robot needs to turn, so the speed of the left and the right wheel need to be in  the opposite directions.\n",
    "\n",
    "\n",
    "2. **Position**: Now that the robot has the right orientation, it needs to move forward to reach the point $P_{k+1}$. The motor command for this part is not computed using a classic P control law because it is based on the angular error. The aim is to have a constant speed when the robot is travelling between two points with the right orientation but also to have a speed close to zero (for this part) when the orientation is wrong : \n",
    "\n",
    "$$\\vec v_{position} = K_{p_d} \\cdot (180 - \\mid\\alpha_e\\mid)$$\n",
    "\n",
    "<img src=\"images/MotionControl_scheme_net.png\" style=\"width: 600px;\">\n",
    "\n",
    "3. **Obstacle avoidance**: This part also sets a speed command using rules that are already described in its own section.\n",
    "\n",
    "These three parts are the main ideas of the motion control approach used for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195ec39",
   "metadata": {},
   "source": [
    "## Implementation problematics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267f3a6",
   "metadata": {},
   "source": [
    "This motion control implementation involved being careful about the following points :\n",
    "   - **Referential consideration** : Two angles need to be considered for this problem, the orientation of the robot $\\alpha_r$ and the angle between the vector $\\vec d(t)$ and the $\\vec y$ axis, which is used to set the orientation command of the robot. This was more difficult than expected to ensure consistence between these two values in any situation.\n",
    "   \n",
    "   \n",
    "   - **Angular discontinuity** : Because an angle is defined between $0 ~rad$ and $2\\pi ~rad$ it creates a huge discontinuity when the robot angle is close to $2\\pi ~rad$ and suddenly goes to $0 ~rad$. For example, suppose the robot needs to reach an orientation of $0 rad$ (which is the discontinuity location) and has a current orientation of $\\pi/6 ~rad$, then the error is $\\alpha_e = \\pi/6 - 0 = 0 ~rad$. The generated motors command will reduce the error to reach $\\alpha_e = 0 ~ rad$ so $\\alpha_r = 0 ~rad$. But if the robot goes further (overshoot) then its orientation will jump from approx. 0 rad to approx. $2\\pi ~rad$. Therefore, the error becomes $\\alpha_e = 2\\pi - 0 = 2\\pi ~rad$ and so the robot will continue to turn on itself. This effect has been avoided by limiting the orientation error $\\alpha_e$ between $-\\pi$ and $\\pi$. For example, if the computed error is $3\\pi/2~rad$ then it is changed to be equal to $-\\pi/2~rad$ which is equivalent.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244a1c8",
   "metadata": {},
   "source": [
    "## Improvement of the motion control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2fd4e",
   "metadata": {},
   "source": [
    "After several trials, we have identified the following problems:\n",
    "   - **Overshoot after reaching a point** : Because of the command law presented previously, the speed of the robot doesn't decrease when it arrives close to its point to reach ($P_{k+1}$). So in this configuration, there is a trade-off between the speed of the robot between two points and the overshoot. To solve this issue, the control law has been improved by modifying the gains $K_{P_\\alpha}$ and $K_{P_d}$ depending if the robot is close to the point to reach or not. If the robot is close (distance <= 20 pixels) to $P_{k+1}$ the gains are $K_{P_d} = 0.5$ and $K_{P_\\alpha} = 4$. If the robot stands further (distance > 20 pixels) to $P_{k+1}$ then the gains are $K_{P_d} = 1$ and $K_{P_\\alpha} = 1$. In this configuration, the robot can easily turn without overshoot when coming close to $P_{k+1}$, because the weight of the orientation command increase relatively to the weight of the position command. It can also move faster when it is far from $P_{k+1}$.\n",
    "\n",
    "\n",
    "   \n",
    "   - **Conflict between $\\vec v_{position}$ and $\\vec v_{orientation}$** : This problem is like the previous one. It first appeared when the robot started its run very close (approximately 10 pixels) to the next point to reach $P_{k+1}$, oriented in the opposite direction of this latter. In this configuration, the robot could not reach the point because the linear speed command was too high in comparison with the rotation command speed. In fact, the robot could only turn around the point. This problem has also been corrected, thanks to the previously shown a modification of the control law, which gives more weight to the orientation command when the robot is close (distance < 20 pixels) to $P_{k+1}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab0254",
   "metadata": {},
   "source": [
    "# VIII . Main\n",
    "<a class=\"anchor\" id=\"Main\"></a>\n",
    "If you desire to execute the code from the notebook, the following cells must be run.\n",
    "1. Instalation of the required packages\n",
    "2. Cloning the github repository\n",
    "3. Modifying the path \n",
    "4. Running the main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instal the required packages, p.ex:\n",
    "# !pip install opencv-python tqdm matplotlib numpy ipywidgets python-dotenv pyserial\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jbyuki/labyrinth-project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd labyrinth-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96248cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main_script.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
